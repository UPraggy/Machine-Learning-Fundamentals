- Today we will look at a small case study
of logistic regression in use.
The problem we'll be solving
is a text classification problem.
So first we will see how a text document
can be converted into a vector of fixed length
so that methods like logistic regression can be run on it.
Then we'll start the learning process
and interpret the results that we get along the way.
So here's the problem we wanna solve.
We get a review of a product or movie or restaurant
and we wanna figure out if it's a positive review
or a negative review.
The data we have is collected from various websites
like Amazon and Yelp, and from each review,
just a single sentence has been extracted
and it's been labeled as plus or minus,
positive or negative.
Okay, so let's look at some of these examples.
So the first one.
Needless to say, I wasted my money.
Okay, so clearly that's a negative review
and the word that really tips you off over there is wasted.
Okay, so that's negative.
The second one.
He was very impressed when going from the original battery
to the extended battery.
So that's positive.
And what is the keyword over there?
Impressed.
The third one.
I had to jiggle the plug in order to get it
to lineup right to get decent volume.
So that's negative.
And in terms of words that really stick out,
there isn't really anything.
There's no single word in there that's completely negative.
So that one's a little bit tricky.
And let's look at the last one.
Will order from them again.
Okay, so that's positive.
And again, there's no single word
that really indicates that.
Again, not really.
There are probably lots of reviews
that say things like will never order from them again.
So that's also a tricky one.
So this is going to be a nontrivial task.
And because it's a little bit difficult,
logistic regression will help us quantify the uncertainty
in some of the predictions.
Since it outputs not just the prediction, plus or minus,
but also gives an accompanying probability.
Now how much data do we have?
About 2,500 training sentences,
which is not very much at all,
and then there's a separate test set of 500 sentences.
Now in order to run logistic regression
we need the data to be vectors of some fixed length.
How do we convert sentences into vectors?
Now there's actually a very standard way
of making documents into vectors,
and this is called the bag-of-words representation.
In our case you can think of each sentence
as being a mini document of some kind.
So here's how it works.
You start by fixing some vocabulary.
For instance, all the words in the documents,
or maybe the 5,000 most common words,
or in some other way you choose a list of words.
For concreteness, let's say that we choose
the 5,000 most common words.
Then each document is represented
as a 5,000 dimensional vector.
We have 5,000 dimensions
with one entry for each of the words in the vocabulary.
And that entry is literally how many times
that word occurs in the document.
So for example, let's say that the first word
in the vocabulary is despair.
So we have a document and what we do is we go through it
and we count how many times despair occurs in the document.
Oh, it occurs once?
So the entry for that is one.
Let's say the next words is evil.
So we go through the document
and see how many times evil occurs in the document.
Oh, two times, so the inquiry is two.
The third word is happiness.
That doesn't occur, so zero.
And so on.
And in this way, we convert this document
into a 5,000 dimensional vector.
Now one thing that's worth pointing out
is that this representation is extremely sparse.
If we're applying this to sentences
and we have a sentence of just 10 words,
then out of those 5,000 inquiries,
at most 10 of them are gonna be nonzero.
So it's a peculiarity of this particular way
of representing documents.
Okay so now we're ready to apply logistic regression.
We have two classes, positive and negative.
Let's code positive as plus one and negative as minus one.
We have our 2,500 data points.
Each of which is now a 5,000 dimensional vector.
And we need to find a linear function,
given by w and b,
that minimizes the logistic regression loss function.
As we saw last time, this is a nice loss function.
It is convex and it can be optimized quite easily
using local search methods.
There are many choices.
Gradient descents, stochastic gradient descent,
Newton-Raphson, and so on and so forth.
They all return the right answer.
So what we'll do today is use one of these,
stochastic gradient descent.
This is a very important method
and this is something that we'll cover
in more detail next week.
But for the time being, it's a local search method.
It starts with some solutions, some guess at w and b,
and then it tweaks it a little bit and it keeps tweaking it
and eventually it converges to the right answer.
So let's see what happens over training iterations.
Now it turns out that for this particular dataset,
300 iterations are needed
before the linear function, w, b, converges.
Each iteration involves a pass through the entire dataset.
And as these iterations proceed,
the loss function on the training set,
the training loss, keeps going down.
And in fact, during the first 50 or so iterations,
the loss function just plunges.
And after that it just tapers off gradually
until it finally grinds to a halt.
So that's how training goes.
And at the end we're left
with our final classifier, w and b.
How does it do?
What is its performance on the test set?
So it turns out that the test error is 21%.
That's not great but it's also not bad
considering the ambiguity and difficulty of these sentences.
So let's go ahead and look at some of the mistakes.
So let's see the first one.
Not much dialogue, not much music,
the whole film was shot as elaborately
and aesthetically like a sculpture.
And that's positive.
It starts off sounding a little bit negative
and there's really no word in there which indicates
that it would be a positive review.
It's not an easy one.
So that's one of the mistakes it made.
Let's look at this one.
The last 15 minutes of move are also not bad as well.
So it's got the double negative, not bad,
which is a little bit confusing.
And so the review is actually positive,
but the logistic regression model thought it was negative.
How about this one?
If you look for authentic Thai food, go elsewhere.
Okay, that's a difficult one.
It's got authentic Thai food in there which sounds good,
but then it says go elsewhere.
So that's difficult.
And the last one, waste your money on this game.
Okay, how are we supposed to get that right?
That's a very difficult one.
Okay, so we've been talking about using logistic regression
for classification, but actually it also outputs
a probability value, which we can think of
as a kind of level of confidence.
What do these probabilities,
what do these confidence levels look like
in this case for this dataset?
On any given sentence x, we get a probability
between zero and one.
And if the probability's above a half,
then we end up predicting plus,
and if the probability is less than a half,
we end up predicting minus.
Now when the probability is close to a half,
it means that we're really very uncertain.
Predicting one way,
we say plus or minus 'cause we're forced to,
but actually there's a lot of uncertainty in there.
The more confident predictions
are the ones that are closer to zero or one.
For example, suppose we look at the cases
where it is above 0.8 or below 0.2.
You can think of this as a sort of 80% confidence.
So we will call this, we'll describe this
as having a margin of 0.3.
What that means is that it's at least 0.3 away from a half.
And this is the formula for margin over here.
It's how far the probability is from a half.
Or we can look at 90% confidence
where the probabilities are between 0.1,
zero and 0.1, or between 0.9 and one.
So this is a margin of 0.4.
Or we can look at even larger margins,
like a margin of 0.49, which would be 99% confidence.
So for what fraction of points was the margin, say, 0.4?
For what fraction of these test points
was there 90% confidence?
Well that's what this graph shows over here.
If we want a margin of 0.4 we just go up over here
and we see that oh, roughly 75% of the points
had a margin of 0.4.
So three quarters of the test points
had 90% confidence or more.
Let's look at 99% confidence.
That's somewhere here.
It looks like roughly 55% of the data had 99% confidence.
That is a lot of confidence.
So this logistic regression model
is making very confident predictions
even though it did not get a whole lot of training data.
That's a little suspicious in and of itself.
And so it would be interesting to see
whether this confidence is at all warranted.
We know that the overall error rate was 21%,
but what if we just look at the points
on which it was confident.
Let's say, the points on which it was 90% confident.
What was the error rate on those points?
And that's what this graph over here shows.
So 90% confidence,
that's when the probability's either greater than 0.9
or less than 0.1 and that's a margin of 0.4.
That's this thing over here.
And if we look at those points,
the error rate was something like 13%.
Much less than 21%.
And if we look at the 99% confident points,
the error rate there is just a little above 10%.
Okay?
So on the test set as a whole, the error rate was 21%.
But more than half the points had a 99% confidence,
and on those points, the error rate was much smaller.
It was just a little over 10%.
This shows two things.
The first thing is that these confidence levels
have to be taken with a grain of salt.
When it's 99% confident, that doesn't mean
that it's going to be correct 99% of the time.
But the second thing is that these confidence values
are really informative.
By focusing on high confidence predictions,
we really can decrease the error rate.
Okay, now let's try and get some other information
about the model that we learned.
What is the basis for its predictions?
What words is it placing the greatest emphasis on?
How do we determine this?
Well, at the end we have this vector w,
which is 5,000 dimensional,
and there's an entry for each word.
So the most significant words,
the ones that have the greatest role in prediction,
are the ones with the largest coefficients.
Either the most positive coefficients
or the most negative coefficients.
What are these words?
Let's take a look.
So here they are.
On top are the words with the largest positive coefficients,
and the bottom are the ones
with the largest negative coefficients.
So what are the positive words?
Beautiful, fantastic, excellent, wonderful,
nice, awesome, perfect, and so on.
And the negative words, disappointing, stupid,
lazy, dirty, bad, fails, unfortunately, and so on.
Do these seem like words that are reasonable indicators
of positive or negative reviews?
They do.
It all seems quite reasonable.
Okay, well that's it for today.
And that's also it for logistic regression.
One of the discussions that we've been putting off
a little bit is how you actually optimize
all of these loss functions.
What are the techniques involved,
and the technology involved in this?
Things like convexity and stochastic gradient descent,
these really lie at the heart of modern machine learning.
So what we're gonna do next is to take a little bit of time
and to study these slowly and carefully.
See you next time.