{"cells":[{"cell_type":"markdown","metadata":{"id":"R2vW7FaYo-Z4"},"source":["# Predicting the progression of diabetes using least-squares regression"]},{"cell_type":"markdown","metadata":{"id":"mvIWVMBAo-Z8"},"source":["The **diabetes** data set described in lecture can be obtained as a single file, `diabetes-data.csv`, from the course website. We obtained it at https://web.stanford.edu/~hastie/Papers/LARS/diabetes.data. For some background information on the data, see this seminal paper:\n","\n","Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n","\n","Before you start on this notebook, install `diabetes-data.csv` in the same directory. We will walk through some of the examples from lecture as well as giving you some problems to solve.\n"]},{"cell_type":"markdown","metadata":{"id":"-9FFlfd2o-Z9"},"source":["## 1. Set up notebook and load data set"]},{"cell_type":"code","source":["!ls \"/content/drive/MyDrive/Colab Notebooks/UCSanDiegoX DSE220x Machine Learning Fundamentals/Week4/sentiment-logistic-regression\"\n","local_folder = \"/content/drive/MyDrive/Colab Notebooks/UCSanDiegoX DSE220x Machine Learning Fundamentals/Week4/sentiment-logistic-regression/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J0yqEOZhphvb","executionInfo":{"status":"ok","timestamp":1749908112359,"user_tz":180,"elapsed":148,"user":{"displayName":"Rafael Moreira Ramos de Rezende","userId":"10933139855840584810"}},"outputId":"05f6b27a-fd01-4bf0-bf34-58c4f0fec31a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["ls: cannot access '/content/drive/MyDrive/Colab Notebooks/UCSanDiegoX DSE220x Machine Learning Fundamentals/Week4/sentiment-logistic-regression': No such file or directory\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qA9VJDpxo-Z-"},"outputs":[],"source":["# Standard includes\n","%matplotlib inline\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","# Routines for linear regression\n","from sklearn import linear_model\n","from sklearn.metrics import mean_squared_error\n","# Set label size for plots\n","matplotlib.rc('xtick', labelsize=14)\n","matplotlib.rc('ytick', labelsize=14)"]},{"cell_type":"markdown","metadata":{"id":"Pa-hsEfHo-Z_"},"source":["This next snippet of code loads in the diabetes data. There are 442 data points, each with 10 predictor variables (which we'll denote `x`) and one response variable (which we'll denote `y`).\n","\n","Make sure the file `'diabetes-data.csv'` is in the same directory as this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y7RkJwjoo-Z_"},"outputs":[],"source":["data = np.genfromtxt(f'{local_folder}diabetes-data.csv', delimiter=',')\n","features = ['age', 'sex', 'body mass index', 'blood pressure',\n","            'serum1', 'serum2', 'serum3', 'serum4', 'serum5', 'serum6']\n","x = data[:,0:10] # predictors\n","y = data[:,10] # response variable"]},{"cell_type":"markdown","metadata":{"id":"SVCsIEARo-aA"},"source":["## 2. Predict `y` without using `x`"]},{"cell_type":"markdown","metadata":{"id":"7pH5Tbwoo-aC"},"source":["If we want to predict `y` without knowledge of `x`, what value would be predict? The <font color=\"magenta\">mean</font> value of `y`.\n","\n","In this case, the mean squared error (MSE) associated with the prediction is simply the variance of `y`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-VQUXZVeo-aD"},"outputs":[],"source":["print(\"Prediction: \", np.mean(y))\n","print(\"Mean squared error: \", np.var(y))"]},{"cell_type":"markdown","metadata":{"id":"9R3nFGQfo-aE"},"source":["## 3. Predict `y` using a single feature of `x`"]},{"cell_type":"markdown","metadata":{"id":"6Wr-l6S9o-aF"},"source":["To fit a linear regression model, we could directly use the formula we saw in lecture. To make things even easier, this is already implemented in `sklearn.linear_model.LinearRegression()`."]},{"cell_type":"markdown","metadata":{"id":"IRLy7BsRo-aF"},"source":["Here we define a function, **one_feature_regression**, that takes `x` and `y`, along with the index `f` of a single feature and fits a linear regressor to `(x[f],y)`. It then plots the data along with the resulting line."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RWpmp46jo-aF"},"outputs":[],"source":["def one_feature_regression(x,y,f):\n","    if (f < 0) or (f > 9):\n","        print(\"Feature index is out of bounds\")\n","        return\n","    regr = linear_model.LinearRegression()\n","    x1 = x[:,[f]]\n","    regr.fit(x1, y)\n","    # Make predictions using the model\n","    y_pred = regr.predict(x1)\n","    # Plot data points as well as predictions\n","    plt.plot(x1, y, 'bo')\n","    plt.plot(x1, y_pred, 'r-', linewidth=3)\n","    plt.xlabel(features[f], fontsize=14)\n","    plt.ylabel('Progression of disease', fontsize=14)\n","    plt.show()\n","    print(\"Mean squared error: \", mean_squared_error(y, y_pred))\n","    return regr"]},{"cell_type":"markdown","metadata":{"id":"Ww5dHD2Ho-aG"},"source":["Let's try this with feature #2 (body mass index)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vy9wPlFUo-aG"},"outputs":[],"source":["regr = one_feature_regression(x,y,2)\n","print(\"w = \", regr.coef_)\n","print(\"b = \", regr.intercept_)"]},{"cell_type":"markdown","metadata":{"id":"wuaW_QDTo-aG"},"source":["<font color=\"magenta\">For you to try:</font> Feature #2 ('body mass index') is the single feature that yields the lowest mean squared error. Which feature is the second best?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ajw1F-Amo-aG"},"outputs":[],"source":["### You can use this space to figure out the second-best feature"]},{"cell_type":"markdown","metadata":{"id":"El1nyow0o-aG"},"source":["## 3. Predict `y` using a specified subset of features from `x`"]},{"cell_type":"markdown","metadata":{"id":"wntTsuCzo-aH"},"source":["The function **feature_subset_regression** is just like **one_feature_regression**, but this time uses a list of features `flist`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V42dWYiQo-aH"},"outputs":[],"source":["def feature_subset_regression(x,y,flist):\n","    if len(flist) < 1:\n","        print(\"Need at least one feature\")\n","        return\n","    for f in flist:\n","        if (f < 0) or (f > 9):\n","            print(\"Feature index is out of bounds\")\n","            return\n","    regr = linear_model.LinearRegression()\n","    regr.fit(x[:,flist], y)\n","    return regr"]},{"cell_type":"markdown","metadata":{"id":"c8AbAnv4o-aH"},"source":["Try using just features #2 (body mass index) and #8 (serum5)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"swTIDe5_o-aH"},"outputs":[],"source":["flist = [2,8]\n","regr = feature_subset_regression(x,y,[2,8])\n","print(\"w = \", regr.coef_)\n","print(\"b = \", regr.intercept_)\n","print(\"Mean squared error: \", mean_squared_error(y, regr.predict(x[:,flist])))"]},{"cell_type":"markdown","metadata":{"id":"F7HeyBiZo-aH"},"source":["Finally, use all 10 features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sjggrSJCo-aI"},"outputs":[],"source":["regr = feature_subset_regression(x,y,range(0,10))\n","print(\"w = \", regr.coef_)\n","print(\"b = \", regr.intercept_)\n","print(\"Mean squared error: \", mean_squared_error(y, regr.predict(x)))"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"tN6z4PRio-aI"},"source":["## 4. Splitting the data into a training and test set"]},{"cell_type":"markdown","metadata":{"id":"BgZQHm0Lo-aI"},"source":["In the experiments above, every model was fit to the *entire* data set and its mean squared error was evaluated on this same data set. This methodology would not, in general, yield accurate estimates of future error. In this specific case, however, the discrepancy might not be too bad because the data set is quite large relative to the number of features."]},{"cell_type":"markdown","metadata":{"id":"Ps1Hv_6No-aI"},"source":["To investigate this further, we define a procedure **split_data** that partitions the data set into separate training and test sets. It is invoked as follows:\n","\n","* `trainx, trainy, testx, testy = split_data(n_train)`\n","\n","Here:\n","* `n_train` is the desired number of training points\n","* `trainx` and `trainy` are the training points and response values\n","* `testx` and `testy` are the test points and response values\n","\n","The split is done randomly, but the random seed is fixed, and thus the same split is produced if the procedure is called repeatedly with the same `n_train` parameter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mphe1cuHo-aI"},"outputs":[],"source":["def split_data(n_train):\n","    if (n_train < 0) or (n_train > 442):\n","        print(\"Invalid number of training points\")\n","        return\n","    np.random.seed(0)\n","    perm = np.random.permutation(442)\n","    training_indices = perm[range(0,n_train)]\n","    test_indices = perm[range(n_train,442)]\n","    trainx = x[training_indices,:]\n","    trainy = y[training_indices]\n","    testx = x[test_indices,:]\n","    testy = y[test_indices]\n","    return trainx, trainy, testx, testy"]},{"cell_type":"markdown","metadata":{"id":"2fA5tII8o-aJ"},"source":["**<font color=\"magenta\">For you to do:</font>** Using the **split_data** procedure to partition the data set, compute the training MSE and test MSE when fitting a regressor to *all* features, for the following training set sizes:\n","* `n_train = 20`\n","* `n_train = 50`\n","* `n_train = 100`\n","* `n_train = 200`"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"toc":{"colors":{"hover_highlight":"#DAA520","navigate_num":"#000000","navigate_text":"#333333","running_highlight":"#FF0000","selected_highlight":"#FFD700","sidebar_border":"#EEEEEE","wrapper_background":"#FFFFFF"},"moveMenuLeft":true,"nav_menu":{"height":"12px","width":"252px"},"navigate_menu":true,"number_sections":false,"sideBar":true,"threshold":4,"toc_cell":false,"toc_section_display":"block","toc_window_display":false,"widenNotebook":false},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}